{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltCxR4pja9MV"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer, Trainer, TrainingArguments, AutoModelForSequenceClassification, ViTForImageClassification\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "# Parameters\n",
        "batch_size = 128\n",
        "max_length = 100\n",
        "vocab_size = 10000\n",
        "embed_dim = 200\n",
        "num_layers = 2\n",
        "num_heads = 4\n",
        "ff_dim = 256\n",
        "num_classes = 2\n",
        "image_size = 224\n",
        "\n",
        "# Transformer Encoder Class\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, max_length, num_layers, num_heads, ff_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.position_embedding = nn.Embedding(max_length, embed_dim)\n",
        "        self.layers = nn.ModuleList([\n",
        "            nn.TransformerEncoderLayer(embed_dim, num_heads, ff_dim, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        positions = torch.arange(x.size(1), device=x.device).unsqueeze(0).expand(x.size(0), -1)\n",
        "        x = self.embedding(x) + self.position_embedding(positions)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "# Dataset Preparation for Text Classification\n",
        "def prepare_text_data():\n",
        "    # Tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "    def preprocess_text(text):\n",
        "        return tokenizer(\n",
        "            text, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "    return preprocess_text\n",
        "\n",
        "# Vision Transformer Class\n",
        "class VisionTransformerCls(nn.Module):\n",
        "    def __init__(self, image_size, embed_dim, num_heads, ff_dim, num_classes, patch_size=16, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.patch_embedding = nn.Conv2d(3, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "        self.positional_embedding = nn.Parameter(torch.randn((image_size // patch_size)**2, embed_dim))\n",
        "        self.transformer_layer = nn.TransformerEncoderLayer(embed_dim, num_heads, ff_dim, dropout)\n",
        "        self.fc = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.patch_embedding(x).flatten(2).permute(0, 2, 1)\n",
        "        x += self.positional_embedding\n",
        "        x = self.transformer_layer(x)\n",
        "        x = x[:, 0, :]\n",
        "        return self.fc(x)\n",
        "\n",
        "# Dataset Preparation for Vision Transformer\n",
        "def prepare_vision_data():\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((image_size, image_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "    ])\n",
        "    dataset = ImageFolder(root=\"./flower_photos\", transform=transform)\n",
        "    num_samples = len(dataset)\n",
        "    train_size = int(num_samples * 0.8)\n",
        "    val_size = int(num_samples * 0.1)\n",
        "    test_size = num_samples - train_size - val_size\n",
        "    train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "    return train_dataset, val_dataset, test_dataset\n",
        "\n",
        "# Training Function\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device):\n",
        "    model.to(device)\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "# Example Usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Text Classification\n",
        "    text_preprocessor = prepare_text_data()\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "    text_model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=num_classes)\n",
        "    text_args = TrainingArguments(output_dir=\"./results\", num_train_epochs=3, per_device_train_batch_size=batch_size)\n",
        "\n",
        "    # Vision Classification\n",
        "    train_dataset, val_dataset, test_dataset = prepare_vision_data()\n",
        "    vision_model = VisionTransformerCls(image_size, embed_dim, num_heads, ff_dim, num_classes)\n",
        "    vision_criterion = nn.CrossEntropyLoss()\n",
        "    vision_optimizer = Adam(vision_model.parameters(), lr=0.001)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "    train_model(vision_model, train_loader, val_loader, vision_criterion, vision_optimizer, num_epochs=5, device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n"
      ]
    }
  ]
}